{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_iris().data\n",
    "y = load_iris().target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[50:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[50:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 4), (112,), (38, 4), (38,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.5, 3. , 5.5, 1.8],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6.7, 2.5, 5.8, 1.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 2. , 3.5, 1. ],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.2, 2.7, 3.9, 1.4]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-05 20:56:18.571516: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Split trainning and test data\n",
    "\n",
    "train = tf.data.Dataset.from_tensor_slices( (X_train, y_train) )\n",
    "train = train.repeat().shuffle(1000).batch(32)\n",
    "\n",
    "test = tf.data.Dataset.from_tensor_slices( (X_test, y_test) )\n",
    "test = test.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and design the network\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # input layer, 4 inputs\n",
    "    tf.keras.Input( (4,) ),\n",
    "    # tow hidden layers, 10 neurons each\n",
    "    tf.keras.layers.Dense( 10, activation=tf.nn.relu ), # activation function is REctivier Linear Unit\n",
    "    tf.keras.layers.Dense( 10, activation=tf.nn.relu ),\n",
    "    # output layer, 3 categories, each represent the probability of that category\n",
    "    tf.keras.layers.Dense( 3 , activation=tf.nn.softmax ), # softmax will turn result to probability\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design the training\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', # method to caculate how wrong the answer is, \n",
    "                                            # then feed to back propagation to modify the weight\n",
    "    optimizer='adam', # a sort of gradient descent, how much the weight should be changed\n",
    "    metrics=['accuracy'] # how the training is progressing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 47/150 [========>.....................] - ETA: 0s - loss: 1.3671 - accuracy: 0.3457 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-05 20:59:08.535216: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 1s 2ms/step - loss: 1.0987 - accuracy: 0.5523 - val_loss: 0.8611 - val_accuracy: 0.6579\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.6944 - accuracy: 0.8017 - val_loss: 0.5416 - val_accuracy: 0.8947\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4057 - accuracy: 0.9504 - val_loss: 0.3214 - val_accuracy: 0.9474\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9681 - val_loss: 0.2198 - val_accuracy: 0.9474\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9712 - val_loss: 0.1746 - val_accuracy: 0.9474\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9698 - val_loss: 0.1659 - val_accuracy: 0.9474\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9773 - val_loss: 0.1584 - val_accuracy: 0.9474\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9765 - val_loss: 0.1189 - val_accuracy: 0.9474\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9783 - val_loss: 0.1315 - val_accuracy: 0.9474\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 965us/step - loss: 0.0637 - accuracy: 0.9802 - val_loss: 0.1005 - val_accuracy: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbfab560dc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training and evaluating\n",
    "\n",
    "model.fit(\n",
    "    train,\n",
    "    validation_data=test,\n",
    "    steps_per_epoch=150, # for each big roop of training data\n",
    "                         # you should do look at (150 examples * 32 batches) flowers\n",
    "    epochs=10            # do 10 big loops\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above:\n",
    "- `- loss: 1.0987 - accuracy: 0.5523` is from the train dataset\n",
    "- `- val_loss: 0.8611 - val_accuracy: 0.6579` is from the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_dict: [9.8634481e-01 1.3655217e-02 1.1081977e-10]\n",
      "🙆‍ Prediction is 'setosa' ( 98.6%), expected setosa\n",
      "pred_dict: [0.00266769 0.98289746 0.01443486]\n",
      "🙆‍ Prediction is 'versicolor' ( 98.3%), expected versicolor\n",
      "pred_dict: [6.8847903e-06 8.1386946e-02 9.1860622e-01]\n",
      "🙆‍ Prediction is 'virginica' ( 91.9%), expected virginica\n"
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "\n",
    "predict_true_labels = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "predict_X = [\n",
    "    [5.1, 3.3, 1.7, 0.5],\n",
    "    [5.9, 3.0, 4.2, 1.5],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "]\n",
    "\n",
    "predictions = model.predict(predict_X)\n",
    "\n",
    "for pred_dict, expected in zip(predictions, predict_true_labels):\n",
    "    print('pred_dict:', pred_dict)  # a list with 3 predicted probabilities for 3 categories as we designed\n",
    "    predicted_index = pred_dict.argmax()\n",
    "    probability = pred_dict.max()\n",
    "    predicted = load_iris().target_names[predicted_index]\n",
    "    \n",
    "    tick_cross = \"🙆‍\" if predicted == expected else \"🙅‍♂️\"\n",
    "    print(f\"{tick_cross} Prediction is '{predicted}' ({100 * probability: .1f}%), expected {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "An image can bee treated as a large grid binary number (between 0 and 255):\n",
    "\n",
    "```\n",
    "# a gray picture, each number is a brightness\n",
    "\n",
    "105 102 100 ...\n",
    "103 109 102 ...\n",
    "99  101 100 ...\n",
    "... ... ... ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Convolution\n",
    "\n",
    "- You can create a kernel which defines a filter to be applied to the image:\n",
    "\n",
    "```\n",
    "# sharpen\n",
    "\n",
    "kernel = [\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "]\n",
    "```\n",
    "\n",
    "- Depending on the values in the kernel different filtering operations will be performed. The most common are:\n",
    "    - sharpen (edge detection)\n",
    "    - blur\n",
    "    - edge detection\n",
    "    \n",
    "- The values of the kernels are created by mathmetical analysis and are generally fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "1. Origin pictur:\n",
    "\n",
    "```\n",
    "105 102 100 ...\n",
    "103 109 102 ...\n",
    "99  101 100 ...\n",
    "... ... ... ...\n",
    "```\n",
    "\n",
    "2. 3 * 3 sharpen kernal\n",
    "\n",
    "```\n",
    "  0  -1   0\n",
    " -1   5  -1\n",
    "  0  -1   0\n",
    "```\n",
    "\n",
    "3. After\n",
    "\n",
    "```\n",
    "# step 1. apply the kernel\n",
    "   0 -102    0\n",
    "-103  545 -102\n",
    "   0 -101    0\n",
    "```\n",
    "\n",
    "```\n",
    "# step 2. calculate\n",
    "Nan   Nan   Nan\n",
    "Nan   137   Nan\n",
    "Nan   Nan   Nan\n",
    "```\n",
    "\n",
    "```\n",
    "# step 3. move the kernel by 1 grid, get the next value\n",
    "Nan   Nan   Nan\n",
    "Nan   137     ?\n",
    "Nan   Nan   Nan\n",
    "```\n",
    "\n",
    "```\n",
    "# Note: 0 can be added outside the edge to calculate the edge grid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwriting Recognition\n",
    "\n",
    "## Convolution Neural Network\n",
    "\n",
    "- Create abstract feature detectors automatically;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINST Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the CNN\n",
    "\n",
    "1. Convolutional Layer #1\n",
    "2. Pooling Layer #1\n",
    "3. Convolutional Layer #2\n",
    "4. Pooling Layer #2\n",
    "5. Dens Layer #1\n",
    "6. Dens Layer #2 (Logist Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
