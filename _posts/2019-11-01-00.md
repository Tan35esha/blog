---
title: Neural Networks and Deep Learning 2-2
published_at: "2019-11-01"
updated_at: "2019-11-01"
author: Taners
tags: [Activation Function, Gradient descent, Random Initialization]
status: published
---

[BACK](../)

# {{page.title}}

by {{page.author}} |
published at {{page.published_at | date: "%Y-%m-%d"}} |
updated at {{ page.updated_at | date: "%Y-%m-%d" }}

---


## Activation Function

- Sigmoid Fuction ( $a = \sigma(z) = \frac{1}{1+e^z}$, which goes between $0$ and $1$ ) is one of the activation function

- In general case we can have different activation function $g(z)$, which could be a non linear function, such as $tanh(z)$, which goes between -1 and 1:

  $$ a = tanh(z) = \frac{e^z - e^z}{e^z + e^z} $$

- **The $tanh(z)$ function is almost always strictly superior**, but one **exception** is for the **output layer**, because if $y$ is either $0$ or $1$, then it makes sense for $\hat{y}$ to be a number, the one to output that's between $0$ and $1$ rather than between $-1$ and $1$

- One of the downsides of both the $\sigma(z)$ and the $tanh(z)$ is that **if $z$ is either very large or very small, then the gradient (derivative, slope) becomes very small**. So one other choice that is very popular in machine learning is what's called the Rectify Linear Unit (ReLU) function, which the derivative (slope) is $0$ when $z$ is negative:

    $$ a = max(0,z) $$

- One disadvantage of the ReLU is that the derivative is equal to $0$, In practice, this works just fine, but here is another version of the ReLU called the leaky ReLU, instead of being $0$ when $z$ is negative, it just takes a slight slope:

    $$ a = max(0.01z,z) $$

- If you're using binary classification, then the $\sigma(z)$ is a very natural choice for the output layer. And then for all other units ReLU is increasingly the **default choice** of activation function. The advantage of both the ReLU and the leaky ReLU is that for a lot of the space of $Z$, the derivative (slope) of the activation function is very different from $0$. And so in practice, using the ReLU activation function, your neural network will often learn much faster than when using the $tanh(z)$ or the $\sigma(z)$. And the main reason is that there is less of these effects of the slope of the function going to $0$, which slows down learning.
  
- ![image1](https://www.diigo.com/file/image/bbosbraozocrcdpsbszdsrrpoce/Activation+functions+-+deeplearning.ai+%7C+Coursera.jpg?k=f2c93b8bebc617e9f3314233b25f5e46)
- [note1](https://diigo.com/0fv32f) 

- ![image2](https://www.diigo.com/file/image/bbosbraozocrcdqsrezdsrrporp/Activation+functions+-+deeplearning.ai+%7C+Coursera.jpg?k=b86b3ee6dc902b6256d0c795d4a10874)
- [note2](https://diigo.com/0fv33t)


### Why do you need non-linear activation functions?

- It turns out that if you use a linear activation function (or alternatively, if you don't have an activation function), then no matter how many layers your neural network has, all it's doing is just computing a linear activation function. So you might as well not have any hidden layers.
- There is just one place where you might use a linear activation function $g(x) = z$. That's if you are doing machine learning on the regression problem, it might be okay to have a linear activation function in the output layer.
  
- ![image](https://www.diigo.com/file/image/bbosbraozocrcdscpqzdsrrpppp/Why+do+you+need+non-linear+activation+functions%3F+-+deeplearning.ai+%7C+Coursera.jpg?k=b32b2f0d6283a2e7a0af1f5e2f4e586e)
- [note](https://diigo.com/0fv3ay)

### Derivatives of activation functions

- $\sigma(z)$:

  - ![image](https://www.diigo.com/file/image/bbosbraozocrceacpdzdsrrpqcp/Derivatives+of+activation+functions+-+deeplearning.ai+%7C+Coursera.jpg?k=13736858a997f8152a3cb6af8e07d1fc)
  - [note](https://diigo.com/0fv3et)
  - The advantage of this formula is that if you've already computed the value for a, then by using this expression, you can very quickly compute the value for the slope for g prime as well.

- $tanh(z)$:

  - ![image](https://www.diigo.com/file/image/bbosbraozocrceaorozdsrrpqoa/Derivatives+of+activation+functions+-+deeplearning.ai+%7C+Coursera.jpg?k=2a8034d8e2faeed13c8df92193f01c7d)
  - [note](https://diigo.com/0fv3gw)
  - The advantage of this formula is that if you've already computed the value for a, then by using this expression, you can very quickly compute the value for the slope for g prime as well.

- ReLU and Leaky ReLU:
  - ![image](https://www.diigo.com/file/image/bbosbraozocrcebadczdsrrpqqe/Derivatives+of+activation+functions+-+deeplearning.ai+%7C+Coursera.jpg?k=11020255a534466278fd0e4808bdf530)
  - [note](https://diigo.com/0fv3il)

### Gradient descent for Neural Networks

- ![image1](https://www.diigo.com/file/image/bbosbraozocrcecbrqzdsrrproc/Gradient+descent+for+Neural+Networks+-+deeplearning.ai+%7C+Coursera.jpg?k=ab261a8960e9845fa75c4d7bf1243fcc)
- [note1](https://diigo.com/0fv3qy)
- ![image2](https://www.diigo.com/file/image/bbosbraozocrcecpqrzdsrrprrd/Gradient+descent+for+Neural+Networks+-+deeplearning.ai+%7C+Coursera.jpg?k=d18c9b28d68793f8bd10ef4f0c413c19)
- [note2](https://diigo.com/0fv3rc)

### Backpropagation intuition (Recap)

- ![image1](https://www.diigo.com/file/image/bbosbraozocrcedosqzdsrrpspr/Backpropagation+intuition+%28optional%29+-+deeplearning.ai+%7C+Coursera.jpg?k=7a3e95d936539a6d51c7111f4cc1ad52)
- [note1](https://diigo.com/0fv4yy)
- ![image2](https://www.diigo.com/file/image/bbosbraozocrcessqrzdsrrqede/Backpropagation+intuition+%28optional%29+-+deeplearning.ai+%7C+Coursera.jpg?k=aa722ebe85977b0d952d21775ca35c44)
- [note2](https://diigo.com/0fv4z1)
- ![image3](https://www.diigo.com/file/image/bbosbraozocrcoacqszdsrrqeee/Backpropagation+intuition+%28optional%29+-+deeplearning.ai+%7C+Coursera.jpg?k=97c98d4b96177a08e15e30fa78f1fe7e)
- [note3](https://diigo.com/0fv4za)
- ![image4](https://www.diigo.com/file/image/bbosbraozocrcoaoqrzdsrrqepp/Backpropagation+intuition+%28optional%29+-+deeplearning.ai+%7C+Coursera.jpg?k=c6f7a3fcc538e0117b4b3d90011793df)
- [note4](https://diigo.com/0fv4zf)

## Random Initialization

- What happens if you initialize weights to zero?
  - It turns out that $dz_1^{[1]}$ and $dz_2^{[1]}$ will be the same,all of these hidden units will initialize the same way.

- Random Initialization:
- ![image](https://www.diigo.com/file/image/bbosbraozocrcocbrazdsrrqooo/Random+Initialization+%7C+Coursera.jpg?k=7c897b65bc98d856429158672a70bc12)
- [note](https://diigo.com/0fv55q)
---

{% for tag in page.tags %}
  {{ tag }}
{% endfor %}

[BACK](../)