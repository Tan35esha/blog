---
title: Improving Deep Neural Networks - Hyperparameter tuning, Regularization and Optimization 1-1
published_at: "2019-11-06"
updated_at: "2019-11-06"
author: Taners
tags: [Regularization, Dropout]
status: published
---

[BACK](../)

# {{page.title}}

by {{page.author}} |
published at {{page.published_at | date: "%Y-%m-%d"}} |
updated at {{ page.updated_at | date: "%Y-%m-%d" }}

---
## Setting up your Machine Learning Application
### Train / Dev / Test sets

- [note1](https://diigo.com/0fwhs1)
- ![image1](https://www.diigo.com/file/image/bbosbraozocrraaqdezdssbspra/Train+%2F+Dev+%2F+Test+sets+-+deeplearning.ai+%7C+Coursera.jpg?k=d347e9ba389c5e5d57f878a35c8b1422)

- [note2](https://diigo.com/0fwhsc)
- ![image2](https://www.diigo.com/file/image/bbosbraozocrrabbqezdssbsqbe/Train+%2F+Dev+%2F+Test+sets+-+deeplearning.ai+%7C+Coursera.jpg?k=64861fe1fbeb61452bdc7decccd202db)

### Bias / Variance

- [note1](https://diigo.com/0fwhyk)
- ![image1](https://www.diigo.com/file/image/bbosbraozocrrabqbozdssbsqes/Bias+%2F+Variance+-+deeplearning.ai+%7C+Coursera.jpg?k=76af995ae7edc826113a2172e7871796)

- [note2](https://diigo.com/0fwhyt)
- ![image2]https://www.diigo.com/file/image/bbosbraozocrracebqzdssbsrcr/Bias+%2F+Variance+-+deeplearning.ai+%7C+Coursera.jpg?k=b09a982fe3c6c74619b159dcc0d0b292

- [note3](https://diigo.com/0fwhz1)
- ![image3](https://www.diigo.com/file/image/bbosbraozocrracqeszdssbsrds/Bias+%2F+Variance+-+deeplearning.ai+%7C+Coursera.jpg?k=01dd85653fb082a1a28b72c4de118f6b)

### Basic Recipe for Machine Learning
- High Bias (training data performance)?
  - Bigger network: more hidden layers or hidden units
  - Train longer
  - NN architecture (may or may not work)
- High Variance (dev set performance)?
  - Get more data
  - Regularization
  - NN architecture (may or may not work)
- Bias and Variance tradeoff if data set is not big enough

## Regularizing your Neural Network
### Logistic Regularization

- To minimize $J(w,b)$

$$ J(w,b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_2^2 + \frac{\lambda}{2m}b^2, \frac{\lambda}{2m}b^2 \approx 0$$

$$ w \in \bold{R}^{n_x \times 1}, b \in \bold{R}$$

- $\lambda$ - ragularization hyper parameter

- **$L_2$ Norm** reqularization:

     $$ ||w||_2^2 =  \sum_{j=1}^{n_x} w_j^2 = w^T w $$

- **$L_1$ Norm** reqularization:

     $$ ||w||_1 =  \sum_{j=1}^{n_x} |w_j|$$


  - $L_2$ is much better then $L_1$, 
  - $L_1$ set many parameters to 0, may work for compressing the modle, but make modle sparse, so don't do that much
  

### Neural Network Regularization

- To minimize $J(w^{[1]},b^{[1]}, ..., w^{[L]},b^{[L]})$

$$ J(w^{[1]},b^{[1]}, ..., w^{[L]},b^{[L]}) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_F^2 $$

- **Forbenius Norm** of the matrix:

     $$ ||w^{[l]}||_F^2 =  \sum_{i=1}^{n^{[l]}} \sum_{j=1}^{n^{[l-1]}} (w_{i,j}^{[l]})^2 = w^T w $$

- **Weight Decay**:
  [note](https://diigo.com/0fwj6a)

  ![image](https://www.diigo.com/file/image/bbosbraozocrrbapsdzdsscadrs/Regularization+-+deeplearning.ai+%7C+Coursera.jpg?k=3d4130b5a0449a18994f574886c5c566)

### Why regularization reduces overfitting?

- If the regularization becomes very large, the parameters $w$ very small, so $z$ will be relatively small, kind of ignoring the effects of b for now, so $z$ will be relatively small or say it takes on a small range of values. And so the activation function, say if is $tanh(z)$, will be relatively linear. And so your whole neural network will be computing something not too far from a big linear function which is pretty simple function rather than a very complex highly non-linear function. And so is also much less able to overfit. 

- Adding extra term $||w^{[l]}||_F^2$ that penalizes the weight being too large

[note](https://diigo.com/0fwjh7)
![image](https://www.diigo.com/file/image/bbosbraozocrrbdecszdsscaobs/Why+regularization+reduces+overfitting%3F+-+deeplearning.ai+%7C+Coursera.jpg?k=a12cc65020a27df80d512da331e7e504)

### Dropout Regularization

[note](https://diigo.com/0fwjk7)
![image](https://www.diigo.com/file/image/bbosbraozocrrbdpsqzdsscaodq/Dropout+Regularization+-+deeplearning.ai+%7C+Coursera.jpg?k=51969bc49cc0a002f4618b1589da712d)

https://diigo.com/0fwjki
https://www.diigo.com/file/image/bbosbraozocrrbecqezdsscaooq/Dropout+Regularization+-+deeplearning.ai+%7C+Coursera.jpg?k=6f4d24a2f4315e8df319890ee75c3dd4

- Do not use Dropout at test time

[note](https://diigo.com/0fwjkm)
![image](https://www.diigo.com/file/image/bbosbraozocrrbeocqzdsscaoqp/Dropout+Regularization+-+deeplearning.ai+%7C+Coursera.jpg?k=93ad90d7580e927ed8136ed1c68a8fbf)

### Understanding Dropout
- layers with a lot of parameters (a lot of units) are easy to overfiting
- different keep props (留存率) for different layers, big hidden layer have smaller keep prop
- One big downside of drop out is that the cost function J is no longer well-defined (monotonically decreasing).

[note](https://diigo.com/0fwjrt)
![image](https://www.diigo.com/file/image/bbosbraozocrrbpbpdzdsscapqp/Understanding+Dropout+-+deeplearning.ai+%7C+Coursera.jpg?k=b13cc884210c786929c9c83a51817676)

### Other regularization methods (数据集扩增)
- [note1](https://diigo.com/0fwjwm)
- ![image1](https://www.diigo.com/file/image/bbosbraozocrrbpsbozdsscaqcb/Other+regularization+methods+-+deeplearning.ai+%7C+Coursera.jpg?k=1cee68c1c072125c84c8428ae3f9ebda)

- [note](https://diigo.com/0fwjws)
- ![image](https://www.diigo.com/file/image/bbosbraozocrrbqeapzdsscaqeb/Other+regularization+methods+-+deeplearning.ai+%7C+Coursera.jpg?k=66492ee3e85de950cf432cdc892f2f9d)
  
---

{% for tag in page.tags %}
  {{ tag }}
{% endfor %}

[BACK](../)