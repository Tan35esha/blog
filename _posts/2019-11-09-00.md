---
title: Improving Deep Neural Networks - Hyperparameter tuning, Regularization and Optimization 2-2
published_at: "2019-11-06"
updated_at: "2019-11-06"
author: Taners
tags: [Momentum]
status: published
---

[BACK](../)

# {{page.title}}

by {{page.author}} |
published at {{page.published_at | date: "%Y-%m-%d"}} |
updated at {{ page.updated_at | date: "%Y-%m-%d" }}

---
## Optimaization Algorithms - 2
### Gradient Descent with Momentum
https://diigo.com/0fxbiq
https://www.diigo.com/file/image/bbosbraozocsbrbqodzdsspopqo/Gradient+descent+with+momentum+-+deeplearning.ai+%7C+Coursera.jpg?k=1e2b4e11113936cb40d0329b6abe1490

https://diigo.com/0fxbis
https://www.diigo.com/file/image/bbosbraozocsbrbseszdsspopsb/Gradient+descent+with+momentum+-+deeplearning.ai+%7C+Coursera.jpg?k=1adc0ee8e4b6c00803fb36bc6dfb24d2

### RMSprop

https://diigo.com/0fxbk2
https://www.diigo.com/file/image/bbosbraozocsbrcoaszdsspoqcq/RMSprop+-+deeplearning.ai+%7C+Coursera.jpg?k=c4d302b59393d52768950648d7d6b5b2

### AdaM (Adaptive Moment estimation) optimization algorithm
https://diigo.com/0fxblh
https://www.diigo.com/file/image/bbosbraozocsbrcsbrzdsspoqes/Adam+optimization+algorithm+-+deeplearning.ai+%7C+Coursera.jpg?k=29f3146b384b0d15efa03598867127a0
https://www.diigo.com/file/image/bbosbraozocsbrcscszdsspoqoc/Clarification+about+Upcoming+Adam+Optimization+Video+%7C+Coursera.jpg?k=a511fecea0372a2ed9ee7f74f136249c

https://diigo.com/0fxblq
https://www.diigo.com/file/image/bbosbraozocsbrdboczdsspoqqp/Adam+optimization+algorithm+-+deeplearning.ai+%7C+Coursera.jpg?k=3aa0fce9ca49d9b3b74e093531b9f7fe

### Learning rate decay

- Slowly reduce $\alpha$

https://diigo.com/0fxbmm
https://www.diigo.com/file/image/bbosbraozocsbrdoppzdssporac/Learning+rate+decay+-+deeplearning.ai+%7C+Coursera.jpg?k=9cde2e6495f5d7208fa499ad35bd87ed

- The formula for learning rate decay is:

    $$ α= \frac{1}{1+decayRate×epochNumber} \alpha_0$$



​	
### 
---

{% for tag in page.tags %}
  {{ tag }}
{% endfor %}

[BACK](../)

```