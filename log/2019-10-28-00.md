---
title: Neural Networks and Deep Learning 1-1
published_at: "2019-10-28"
updated_at: "2019-10-28"
author: Taners
tags: [Binary Classifiation, Logistic Regression, Gradient Descent, Derivative, Forward Propagation, Backward Propagation]
status: published
---

[BACK](../)

# {{page.title}}

by {{page.author}} |
published at {{page.published_at | date: "%Y-%m-%d"}} |
updated at {{ page.updated_at | date: "%Y-%m-%d" }}

---


## Eg.

- Input: a picture of RGB ($3$ chanles) of a certain ppi, such as $64\times64$

- Output: $1$ (cat) vs. $0$ (non cat)

- Total number of input samples: $m$


## Binary Classification

- Given feature vector x: 
  
  $$ x\in \boldsymbol{R}^{n^{[x]} \times 1} $$

- Output binary y: 

  $$ y \in \{0,1\} $$

  ---

- Number of features in a single sample $x$, whose `ppi` is $64 \times 64 \times 3$: 
  
  $$ n^{[x]} = 64 \times 64 \times 3 $$

- Feature matrix of sample $x$:
  
  $$ x = [r_1, r_2, ..., r_{4096}, g_1, g_2, ..., g_{4096}, b_1, b_2, ..., b_{4096}]^T $$

- ![image1](https://www.diigo.com/file/image/bbosbraozocqqapbpazdsrobera/Binary+Classification+-+deeplearning.ai+%7C+Coursera.jpg?k=556555acfb39693a610c1864f942b02a)
- [note](https://diigo.com/0ftz6m)



- A single sample is represented by a pair:
  
  $$ (x, y) $$

  $$ x \in R^{n^{[x]} \times 1}, y \in \{0, 1\} $$

- Number of training examples is $m$, samples set is:
  
  $$ \{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\} $$

- Feature matrix of all samples:
  
  $$ X = [x_1, x_2, ..., x_m] $$

  $$ X \in \boldsymbol{R}^{n^{[x]} \times m} $$

- Output matrix:
  
  $$ Y = [y_1, y_2, ..., y_m] $$

  $$ Y \in \boldsymbol{R}^{1 \times m}


- Size of test or train set:
  $m_{test}$, $m_{train}$

- ![image2](https://www.diigo.com/file/image/bbosbraozocqqaqooezdsrobpcb/Binary+Classification+-+deeplearning.ai+%7C+Coursera.jpg?k=e474bb0dcda6e3e7293040cb6d50d0f0)
- [note](https://diigo.com/0ftzal)
  
## Logistic Regression

- Given feature vector x: 
  
  $$ x \in \boldsymbol{R}^{n^{[x]} \times 1} $$

- Output - prediction the estimate chance that $y=1$: 
  
  $$ \hat{y}=P(y=1|x), 0 \leq \hat{y} \leq 1 $$

  ---

- Given parameters $w$ and $b$:
  
  $$ w \in \boldsymbol{R}^{n^{[x]} \times 1}, b \in \boldsymbol{R} $$

- Output - sigmoid function: 
  
  $$ \hat{y} = \sigma ({w^T}  {x} + b) $$

  $$ \sigma (z) = \frac{1}{(1+e)^{-z}} $$

  $$ z = {w^T}  {x} + b $$

  
- ![image](https://www.diigo.com/file/image/bbosbraozocqqadqaezdsrobcoc/Logistic+Regression+-+deeplearning.ai+%7C+Coursera.jpg?k=a709fe8753e4083d005fa871a3a9a971)
- [note](https://diigo.com/0ftz0n)


## Logistic Regression Cost Function
- Loss (error) function - for single training example:
  - In some senarios, it may be like:
  
  $$ L(y, \hat{y}) = \frac{1}{2} (\hat{y} - y)^2 $$

  - While, in Logistic Regression:
  
  $$ L(y, \hat{y}) = -(ylog{\hat{y}} + (1-y)log(1-{\hat{y}})) $$

- Cost function - for entire training set:

  $$ J(w,b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y_i}, y_i) = -\frac{1}{m} \sum_{i=1}^{m} (y_ilog{\hat{y_i}} + (1-y_i)log(1-{\hat{y_i}})) $$

- ![image](https://www.diigo.com/file/image/bbosbraozocqqbbdeczdsrobsee/Logistic+Regression+Cost+Function+-+deeplearning.ai+%7C+Coursera.jpg?k=9d8d8b63d90d5ad90bf475ec2b25877b)
- [note](https://diigo.com/0ftznd)


## Gradient Descent

- Want to find $w$ and $b$ that minimize $J(w,b)$ (convex function)

- ![image1](https://www.diigo.com/file/image/bbosbraozocqqcbabozdsrocroq/Gradient+Descent+-+deeplearning.ai+%7C+Coursera.jpg?k=a060ff81b59fa8dea4a574e141a42dc0)
- [note1](https://diigo.com/0fu0k6)

**- Forward propagation to compute Cost Function**

- Repeat until the algorithm converges:
  - For one parameter (1-dimention) function $J(w)$ - derivative:
    - $w := w - \alpha \frac{\mathrm{d}J(w)}{\mathrm{d}w}$
      - $:=$ means update
      - $\alpha$ is learning rate
      - $\frac{\mathrm{d}J(w)}{\mathrm{d}w}$, simply `"dw"` in code, is $derivative$ (slope), is the change you want to make to the parameter $w$

  - For two prameter (2-dimention) function $J(w,b)$ - partial derivative:
    - $w := w - \alpha \frac{\partial{J(w,b)}}{\partial{w}}$
      - $\frac{\partial{J(w,b)}}{\partial{w}}$, simply `"dw"` in code
    - $b := w - \alpha \frac{\partial{J(w,b)}}{\partial{b}}$
      - $\frac{\partial{J(w,b)}}{\partial{b}}$, simply `"db"` in code

- ![image2](https://www.diigo.com/file/image/bbosbraozocqqceoqbzdsroddad/Gradient+Descent+-+deeplearning.ai+%7C+Coursera.jpg?k=af1509682ee4332dd20de3f89554b2fd)
- [note2](https://diigo.com/0fu0tp)

### Derivative
- ![image1](https://www.diigo.com/file/image/bbosbraozocqqdsqrqzdsrooccc/Derivatives+-+deeplearning.ai+%7C+Coursera.jpg?k=5fa370d13bc385a1b68662a01636605e)
- [note1](https://diigo.com/0fu24b)

- ![image2](https://www.diigo.com/file/image/bbosbraozocqqdssrbzdsrooced/More+Derivative+Examples+-+deeplearning.ai+%7C+Coursera.jpg?k=e4cf1c9475a1ec93bdd1b84bc0938e48)
- [note2](https://diigo.com/0fu24x)

- ![image3](https://www.diigo.com/file/image/bbosbraozocqqeabbczdsroococ/More+Derivative+Examples+-+deeplearning.ai+%7C+Coursera.jpg?k=b570b830b5a495f6ea0ec1f2def8fe18)
- [note3](https://diigo.com/0fu252)


### Computation Graph

- Backward Propagation
  
- ![image](https://www.diigo.com/file/image/bbosbraozocqqcrapdzdsrodore/Computation+graph+-+deeplearning.ai+%7C+Coursera.jpg?k=edf6ef41b33496de4fbb5e9830788761)
- [note](https://diigo.com/0fu13r)

### Derivative with a Computation Graph

**- Backward Propagation is more efficient way to compute derivative**

- ![image](https://www.diigo.com/file/image/bbosbraozocqqdrcbbzdsrooapp/Derivatives+with+a+Computation+Graph+-+deeplearning.ai+%7C+Coursera.jpg?k=3e9437356c71c491e8029261138f959b)
- [note]https://diigo.com/0fu1zm

- ![image](https://www.diigo.com/file/image/bbosbraozocqqdsaaazdsroobap/Derivatives+with+a+Computation+Graph+-+deeplearning.ai+%7C+Coursera.jpg?k=9dc20a3219c7b511c2e7e9170ec19003)
- [note](https://diigo.com/0fu220)


### Logistic Regression Gradient Descent - Loss Function

- Logistic regression recap:
  - $z = w^T x + b$
  - $\hat{y} = \sigma (z)$
  - $L(\hat{y}, y) = -(ylog(\hat{y}) + (1-y)log(1-\hat{y}))$ 

- To minimize $L(\hat{y}, y)$, what is $w$ and $b$: take two $x$ features as example:

  $$ x = [x_1, x_2]^T, \hat{y} = a $$
  $$ w = [w_1, w_2]^T, b \in \boldsymbol{R} $$
  
  - what is `dw1`, `dw2`, and `db`
  - ![image](https://www.diigo.com/file/image/bbosbraozocqqedcaozdsrooooq/Logistic+Regression+Gradient+Descent+-+deeplearning.ai+%7C+Coursera.jpg?k=b5804db7e7c8a2b1ee429b7063df073b)
  - [note](https://diigo.com/0fu2c7)

### Gradient Descent on m Examples - Cost Function

- Logistic regression recap:
  
  - $$ J(w,b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y_i}, y_i) = -\frac{1}{m} \sum_{i=1}^{m} (y_ilog{\hat{y_i}} + (1-y_i)log(1-{\hat{y_i}})) $$

- Backward and Forward Propagation in `for` loop, while **vectorization** can implement it **without loop**
- ![image1](https://www.diigo.com/file/image/bbosbraozocqqeoaarzdsrooqco/Gradient+Descent+on+m+Examples+-+deeplearning.ai+%7C+Coursera.jpg?k=a0b157a10c99f6fafcc8c27d34e93b63)
- [note1](https://diigo.com/0fv2i8)
- ![image2](https://www.diigo.com/file/image/bbosbraozocqqepaadzdsrooqra/Gradient+Descent+on+m+Examples+-+deeplearning.ai+%7C+Coursera.jpg?k=0e30fe855dd85f71e1818a7c25dd3369)
- [note2](https://diigo.com/0fu2ic)

- [detail1](https://www.coursera.org/learn/neural-networks-deep-learning/supplement/chdB6/derivation-of-dl-dz-part-1)

- [detail2](https://www.coursera.org/learn/neural-networks-deep-learning/supplement/vl2Pa/derivation-of-dl-dz-part-2)





---

{% for tag in page.tags %}
  {{ tag }}
{% endfor %}

[BACK](../)