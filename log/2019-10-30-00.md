---
title: Neural Networks and Deep Learning 1-2
published_at: "2019-10-30"
updated_at: "2019-10-30"
author: Taners
tags: [Vectorization, Broadcsting]
status: published
---

[BACK](../)

# {{page.title}}

by {{page.author}} |
published at {{page.published_at | date: "%Y-%m-%d"}} |
updated at {{ page.updated_at | date: "%Y-%m-%d" }}

---
## Vectorization

- What is vectorization:
  - Suppose you have vecter $w, x$, where:
  
    $$ z = {w^T}  {x} + b $$

    $$ w \in R^{n_x \times 1}, x \in R^{n_x \times 1}, b \in R $$

  - Non-vectorized implementation:

    ```python
    z = 0
    for i in range (n_x):
      z += w[i] * x[i]
    z += b
    ```
  
  - Vectorized implementation:

    ```python
    z = np.dot(w, x) + b    # much faster than non-vectorized implementation
    ```
  - example 1:

    ```python
    a = np.random.rand(1000000)
    b = np.random.rand(1000000)

    # Non-vectorized implementation
    c = 0
    tic = time.time()
    for i in range(1000000):
      c += a[i]*b[i]
    toc = time.time()
    print(c)

    time_consumed = 1000 * (toc-tic)
    print('Time consumed for non-vectorized version: {} ms'.format(time_consumed))

    # Vectorized implementation
    tic = time.time()
    c = np.dot(a,b)
    toc = time.time()
    print(c)

    time_consumed = 1000 * (toc-tic)
    print('Time consumed for vectorized version: {} ms'.format(time_consumed))
    ```

- Neural network programming guideline:
  
  - whenever possible, avoid explicit for-loops.

  - Both GPU and CPU have parallelization instructions, sometimes called SIMD (Single Instruction Multiple Data) instructions, enable vectorized implementation

  - Example 2:

    you have a vector $v$:

    $$ v = [v_1, v_2, ..., v_n]^T $$
  
    you want to apply the exponential operation on everay element of $v$:

    $$ u = [e^{v_1}, e^{v_2}, ..., e^{v_n}]^T $$
  
    ```python
    # Non-vectorized implementation
    u = np.zeros((n,1))
    for i in range(n):
      u[i] = math.exp(v[i])

    # Vectorized implementation
    u = np.exp(v)

    # Other utilities of SIMD instructions in np
    np.log()
    np.abs()
    np.maximum(v,0)
    v ** 2
    1/v 
    ```

### Vectorizing Logistic Regression

- Logistic regression recap:
  - $z = w^T x + b$
  - $\hat{y} = \sigma (z)$
  - $L(\hat{y}, y) = -(ylog(\hat{y}) + (1-y)log(1-\hat{y}))$
  - $$ J(w,b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y_i}, y_i) = -\frac{1}{m} \sum_{i=1}^{m} (y_ilog{\hat{y_i}} + (1-y_i)log(1-{\hat{y_i}})) $$

- Logistic regression derivatives:

  - ![image1](https://www.diigo.com/file/image/bbosbraozocqsodpodzdsrpsepo/More+Vectorization+Examples+-+deeplearning.ai+%7C+Coursera.jpg?k=8550d825b09541f5a2bdd99f74927fed)
  - [note1](https://diigo.com/0fv2cl)

- Vectorizing Logistic Regression - Forward Propagation:

  - ![image2](https://www.diigo.com/file/image/bbosbraozocqsodccazdsrpsedd/Vectorizing+Logistic+Regression+-+deeplearning.ai+%7C+Coursera.jpg?k=b8a42fdb375179f466c36a0a8d8415f5)
  - [note2](https://diigo.com/0fue8n)

- Vectorizing Logistic Regression's Gradient Output - Backward Propagation:
  
  - ![image3](https://www.diigo.com/file/image/bbosbraozocraqoaeqzdsrqpsca/Vectorizing+Logistic+Regression%27s+Gradient+Output+-+deeplearning.ai+%7C+Coursera.jpg?k=b14f2a9220252eb0869cf7d27b1fe643)
  - [note3](https://diigo.com/0fumtx)

  - ![image4](https://www.diigo.com/file/image/bbosbraozocraqodrrzdsrqpsep/Vectorizing+Logistic+Regression%27s+Gradient+Output+-+deeplearning.ai+%7C+Coursera.jpg?k=1f99b49fc210bcd569139c6bf9f87c56)
  - [note4](https://diigo.com/0fumuq)


## Broad Casting

- Example: Calculate percentage of calories from carbs, protein, fat, do this without for loop.

  - Calories from carbs, proteins, fats in 100g of different food:

    -|Apple|Beef|Eggs|Potatoes
    -|-|-|-|-
    Carb|56.0|0.0|4.4|68.0
    Protein|1.2|104.0|52.0|8.0
    Fat|1.8|135.0|99.0|0.9

  ```python
  A = np.array([[56.0, 0.0, 4.4, 68.0],
              [1.2, 104.0, 52.0, 8.0],
              [1.8, 135.0, 99.0, 0.9]])
  display(A)

  cal = A.sum(axis=0)
  display(cal)

  percent = 100 * (A/cal.reshape(1,4)) # just make sure the shape of cal
  display(percent)
  ```

- Other Examples:

  - ![image](https://www.diigo.com/file/image/bbosbraozocraracpczdsrqqbdb/Broadcasting+in+Python+-+deeplearning.ai+%7C+Coursera.jpg?k=423083dc61841050daf6071e128363ee)
  - [note](https://diigo.com/0funaz)

- General Priciple:
  
  $$ (m, n) \{+,-,*,/\}
  \begin{cases}
    (1, n) \\
    (m, 1) \\
    \boldsymbol{R}
  \end{cases}
  \Rightarrow (m, n)
  $$

- Take Caution when Coding!

```python
# Take Caution!

a = np.random.rand(5) # rank 1 array, DO NOT USE THIS as DATA STRUCTURE!

display(a)
print(a.shape)  # shap (5,)

display(a.T)
print(a.T.shape)  # shap (5,)

display(np.dot(a, a.T))   # (5,) dot (5,) ?

# This way is better!
a = np.random.rand(5,1)

display(a)
print(a.shape)    # shap (5,1), row vecter

display(a.T)
print(a.T.shape)  # shap (1,5), column vecter

# OR
assert(a.shape == (5,1))
# OR 
a.reshape((5,1))
```
---

{% for tag in page.tags %}
  {{ tag }}
{% endfor %}

[BACK](../)